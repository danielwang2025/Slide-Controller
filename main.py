import cv2
import mediapipe as mp
import time
import pyautogui
import threading
import numpy as np
import sounddevice as sd
from faster_whisper import WhisperModel
from collections import deque

# åˆå§‹åŒ– Whisper æ¨¡åž‹
model = WhisperModel("small", compute_type="int8")

# åˆå§‹åŒ– MediaPipe æ‰‹åŠ¿è¯†åˆ«
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)
mp_draw = mp.solutions.drawing_utils

# æ‘„åƒå¤´åˆå§‹åŒ–
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("âŒ æ‘„åƒå¤´æ— æ³•æ‰“å¼€")
    exit()

# ç¿»é¡µå‡½æ•°
def go_to_next_slide():
    print("âž¡ï¸ ç¿»åˆ°ä¸‹ä¸€é¡µ")
    pyautogui.press('right')

def go_to_previous_slide():
    print("â¬…ï¸ å›žåˆ°ä¸Šä¸€é¡µ")
    pyautogui.press('left')

# éŸ³é¢‘ç¼“å­˜å’ŒæŽ§åˆ¶å‚æ•°
samplerate = 16000
block_duration = 0.5  # æ¯æ¬¡é‡‡é›†0.5ç§’
buffer_seconds = 5
audio_buffer = deque(maxlen=int(buffer_seconds / block_duration))  # ä¿å­˜æœ€è¿‘5ç§’éŸ³é¢‘å—

# éŸ³é¢‘é‡‡é›†çº¿ç¨‹
def audio_capture():
    def callback(indata, frames, time_info, status):
        if status:
            print("âš ï¸", status)
        audio_buffer.append(np.copy(indata[:, 0]))  # åªå–1é€šé“

    with sd.InputStream(channels=1, samplerate=samplerate, callback=callback, blocksize=int(samplerate * block_duration)):
        print("ðŸŽ™ï¸ å¼€å§‹éŸ³é¢‘é‡‡é›†...")
        while True:
            sd.sleep(100)

# éŸ³é¢‘è¯†åˆ«çº¿ç¨‹
def audio_recognition():
    print("ðŸ¤– å¼€å§‹è¯­éŸ³è¯†åˆ«ï¼ˆè¯´ 'next slide' æˆ– 'go back'ï¼‰")
    while True:
        if len(audio_buffer) == 0:
            time.sleep(0.5)
            continue

        audio_data = np.concatenate(list(audio_buffer))
        audio_buffer.clear()  # æ¸…ç©ºç¼“å­˜ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡

        # æŽ¨ç†
        segments, _ = model.transcribe(audio_data, language="en", beam_size=5)
        for segment in segments:
            command = segment.text.strip().lower()
            print(f"ðŸ—£ï¸ è¯†åˆ«ç»“æžœï¼š{command}")
            if "next slide" in command:
                go_to_next_slide()
            elif "go back" in command:
                go_to_previous_slide()

# å¯åŠ¨è¯­éŸ³çº¿ç¨‹
threading.Thread(target=audio_capture, daemon=True).start()
threading.Thread(target=audio_recognition, daemon=True).start()

# æ‰‹åŠ¿ç¿»é¡µå˜é‡
prev_x = None
last_action_time = 0
cooldown = 1  # ç§’

# ä¸»å¾ªçŽ¯ï¼ˆæ‰‹åŠ¿æŽ§åˆ¶ï¼‰
while True:
    success, img = cap.read()
    if not success:
        print("âŒ æ‘„åƒå¤´è¯»å–å¤±è´¥")
        break

    img = cv2.flip(img, 1)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    results = hands.process(img_rgb)

    if results.multi_hand_landmarks:
        for handLms in results.multi_hand_landmarks:
            mp_draw.draw_landmarks(img, handLms, mp_hands.HAND_CONNECTIONS)

            x = handLms.landmark[0].x
            current_time = time.time()

            if prev_x is not None and (current_time - last_action_time > cooldown):
                dx = x - prev_x
                if dx > 0.1:
                    go_to_next_slide()
                    last_action_time = current_time
                elif dx < -0.1:
                    go_to_previous_slide()
                    last_action_time = current_time

            prev_x = x

    cv2.imshow("ðŸŽ¬ Slide Controller (æ‰‹åŠ¿ + è¯­éŸ³)", img)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
